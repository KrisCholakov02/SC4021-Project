{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SC4021 - Data Collection, Cleaning and Analysis\n",
    "This notebook presents the data collection, cleaning and analysis for the SC4021 - Information Retrieval course project. We will start with the data collection from Reddit using the Python Reddit API Wrapper (PRAW)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "997efd316f30bb09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Reddit Data Collection\n",
    "In this section, we will collect the data from Reddit using the Python Reddit API Wrapper (PRAW). We will collect the submissions and comments for the following subreddits:\n",
    "- VisionPro\n",
    "- virtualreality\n",
    "- augmentedreality\n",
    "- MetaQuestVR\n",
    "- oculus\n",
    "- OculusQuest\n",
    "We will collect the data for the top 1000 submissions for each subreddit (the collection is limited to 1000 submissions due to the Reddit API limitations).\n",
    "\n",
    "But first, we need to install and import the necessary libraries. We will install the PRAW library using the following command: `pip install praw`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fb85f2de43f864e"
  },
  {
   "cell_type": "code",
   "source": [
    "# Importing the necessary libraries\n",
    "import praw  # Python Reddit API Wrapper\n",
    "import pandas as pd # Data manipulation library\n",
    "import os # Operating system library\n",
    "from datetime import datetime # Datetime library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0dc2296ee867cd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After we have successfully installed the necessary libraries, we can proceed with the data collection. First, we will define the constants and the Reddit instance.",
   "id": "babffa0cb0175345"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the constants and the Reddit instance\n",
    "\n",
    "# Create a Reddit instance\n",
    "reddit = praw.Reddit(user_agent=True, client_id='kqB2Mfaq32Jax9LAmdsr3A',\n",
    "                     client_secret='7HR4TNjSVDXZrgEwlsrjF0Pcwzdc2w', username='KrisCholakov',\n",
    "                     password='kyhnoh-pixci0-Bedgit')\n",
    "\n",
    "# Define the columns for the submissions and comments dataframes\n",
    "submission_columns = [\"author\", \"created_utc\", \"distinguished\", \"id\", \"name\", \"num_comments\", \"score\", \"selftext\", \"title\", \"upvote_ratio\", \"url\"]\n",
    "comment_columns = [\"author\", \"body\", \"body_html\", \"created_utc\", \"distinguished\", \"id\", \"link_id\", \"parent_id\", \"score\"]\n",
    "\n",
    "# Define the directory to save the data for the subreddits\n",
    "data_directory = \"subreddits\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6655cc5a35558dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we will define the functions to crawl the data from Reddit. We will define the function to get the submissions and the corresponding comments for a subreddit.",
   "id": "816fed0507bc4533"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the functions to craw the data from Reddit\n",
    "\n",
    "# Function to get the submissions and the corresponding comments for a subreddit\n",
    "def get_submissions_and_comments(reddit, subreddit_name, limit=None):\n",
    "    # Create the lists to store the submissions and comments\n",
    "    submissions_list = []\n",
    "    comments_list = []\n",
    "    \n",
    "    # Define the counter for the submissions and comments\n",
    "    submission_cnt, comment_cnt = 1, 1\n",
    "    \n",
    "    # Browse the submissions\n",
    "    for submission in reddit.subreddit(subreddit_name).top(limit=limit):\n",
    "        # Print the progress - submission title, submission cnt\n",
    "        print(f'{subreddit_name}-{submission_cnt}', submission.title)\n",
    "        # Define the submission\n",
    "        new_submission = {\n",
    "        \"author\": submission.author,\n",
    "        \"created_utc\": submission.created_utc,\n",
    "        \"distinguished\": submission.distinguished,\n",
    "        \"id\": submission.id,\n",
    "        \"name\": submission.name,\n",
    "        \"num_comments\": submission.num_comments,\n",
    "        \"score\": submission.score,\n",
    "        \"selftext\": submission.selftext,\n",
    "        \"title\": submission.title,\n",
    "        \"upvote_ratio\": submission.upvote_ratio,\n",
    "        \"url\": submission.url\n",
    "    }\n",
    "        # Add the submission to the list\n",
    "        submissions_list.append(new_submission)\n",
    "        # Get the comments\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        # Browse the comments\n",
    "        for comment in submission.comments.list():\n",
    "            # Print the progress - comment cnt\n",
    "            print(f'comment #{comment_cnt}')\n",
    "            # Define the comment\n",
    "            new_comment = {\n",
    "            \"author\": comment.author,\n",
    "            \"body\": comment.body,\n",
    "            \"body_html\": comment.body_html,\n",
    "            \"created_utc\": comment.created_utc,\n",
    "            \"distinguished\": comment.distinguished,\n",
    "            \"id\": comment.id,\n",
    "            \"link_id\": comment.link_id,\n",
    "            \"parent_id\": comment.parent_id,\n",
    "            \"score\": comment.score\n",
    "        }\n",
    "            # Add the comment to the list\n",
    "            comments_list.append(new_comment)\n",
    "            comment_cnt += 1\n",
    "        submission_cnt += 1\n",
    "\n",
    "    # Convert the lists to dataframes\n",
    "    submissions = pd.DataFrame(submissions_list, columns=submission_columns)\n",
    "    comments = pd.DataFrame(comments_list, columns=comment_columns)\n",
    "    \n",
    "    return submissions, comments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "788cbe83f2f92dd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After having defined the function to crawl the data from Reddit we will define some helper functions to save the data to csv files in the corresponding directory.",
   "id": "53a770928071c5a8"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the functions to save the data to csv files in the corresponding directory\n",
    "\n",
    "# Function to save the submissions and comments to csv files\n",
    "def save_submissions_and_comments(submissions, comments, subreddit_name):\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(f'{data_directory}/{subreddit_name}'):\n",
    "        os.makedirs(f'{data_directory}/{subreddit_name}')\n",
    "    \n",
    "    # Save the submissions to a csv file\n",
    "    submissions.to_csv(f'{data_directory}/{subreddit_name}/submissions.csv', index=False)\n",
    "    # Save the comments to a csv file\n",
    "    comments.to_csv(f'{data_directory}/{subreddit_name}/comments.csv', index=False)\n",
    "    \n",
    "# Function to check if the subreddit directory exists and if the data is already collected\n",
    "def check_subreddit_data(subreddit_name):\n",
    "    # Check if the subreddit directory exists\n",
    "    if not os.path.exists(f'{data_directory}/{subreddit_name}'):\n",
    "        return False\n",
    "    # Check if the submissions and comments csv files exist\n",
    "    if not os.path.exists(f'{data_directory}/{subreddit_name}/submissions.csv') or not os.path.exists(f'{data_directory}/{subreddit_name}/comments.csv'):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d22bab52ee1b672e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Having defined all the functions needed for the crawling and saving the data, we can now proceed with the data collection for the subreddits. But first, let's define the subreddits we want to crawl the data from.",
   "id": "7f56085a31334426"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the subreddits to crawl the data from\n",
    "subreddits = [\"VisionPro\", \"virtualreality\", \"augmentedreality\", \"MetaQuestVR\", \"oculus\", \"OculusQuest\"]"
   ],
   "id": "6b3717e517831cad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's crawl the data for the subreddits.",
   "id": "7b7a97dae020b9db"
  },
  {
   "cell_type": "code",
   "source": [
    "# Crawl the data for the subreddits\n",
    "for subreddit_name in subreddits:\n",
    "    # Check if the data is already collected\n",
    "    if check_subreddit_data(subreddit_name):\n",
    "        continue\n",
    "    # Get the submissions and comments\n",
    "    submissions, comments = get_submissions_and_comments(reddit, subreddit_name, limit=1000)\n",
    "    # Save the data to csv files\n",
    "    save_submissions_and_comments(submissions, comments, subreddit_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8fb19bb303aca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Analyzing the data\n",
    "In this section, we will analyze the collected data. This section is important to understand the data and to identify any issues that need to be addressed in the data cleaning section. \n",
    "\n",
    "First, we will need to import the matplotlib library, used to visualize the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ffcf332f1059ee1"
  },
  {
   "cell_type": "code",
   "source": [
    "# Importing the necessary libraries\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5164bc4c4a6928e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we will define the functions to load the data and analyze it.",
   "id": "dde5cd9d4deeafc3"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the functions to load the data and analyze it\n",
    "\n",
    "# Function to load the submissions and comments dataframes\n",
    "def load_submissions_and_comments(subreddit_name):\n",
    "    # Load the submissions and comments dataframes\n",
    "    submissions = pd.read_csv(f'{data_directory}/{subreddit_name}/submissions.csv')\n",
    "    comments = pd.read_csv(f'{data_directory}/{subreddit_name}/comments.csv')\n",
    "    \n",
    "    return submissions, comments\n",
    "\n",
    "def simple_analyze_submissions_and_comments(submissions, comments):\n",
    "    # Create a dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    # Calculate and store the results in the dictionary\n",
    "    results[\"Number of submissions\"] = len(submissions)\n",
    "    results[\"Number of comments\"] = len(comments)\n",
    "    results[\"Number of unique authors in submissions\"] = len(submissions[\"author\"].unique())\n",
    "    results[\"Number of unique authors in comments\"] = len(comments[\"author\"].unique())\n",
    "    results[\"Number of unique submissions\"] = len(submissions[\"id\"].unique())\n",
    "    results[\"Number of unique comments\"] = len(comments[\"id\"].unique())\n",
    "    comments[\"word_length\"] = comments[\"body\"].apply(lambda x: len(str(x).split()))\n",
    "    results[\"Average word length of comments\"] = comments[\"word_length\"].mean()\n",
    "    results[\"Number of comments that have more than 50 words\"] = len(comments[comments[\"word_length\"] > 50])\n",
    "    results[\"Number of submissions that have more than 50 words in the selftext\"] = len(submissions[submissions[\"selftext\"].apply(lambda x: len(str(x).split())) > 50])\n",
    "    results[\"Average score of submissions\"] = submissions[\"score\"].mean()\n",
    "    results[\"Average score of comments\"] = comments[\"score\"].mean()\n",
    "    results[\"Average number of comments per submission\"] = submissions[\"num_comments\"].mean()\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    results = pd.DataFrame(list(results.items()), columns=['Description', 'Data'])\n",
    "    \n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d63d55dce97988",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we will combine the data for all subreddits and perform the simple analysis.",
   "id": "eda7b8613bc1f3fd"
  },
  {
   "cell_type": "code",
   "source": [
    "# Perform analysis on the combined data for all subreddits\n",
    "\n",
    "# Load the data for the subreddits and analyze it\n",
    "submissions_list = []\n",
    "comments_list = []\n",
    "for subreddit_name in subreddits:\n",
    "    # Load the submissions and comments dataframes\n",
    "    submissions, comments = load_submissions_and_comments(subreddit_name)\n",
    "    # Add the subreddit name to the submissions and comments dataframes\n",
    "    submissions[\"subreddit\"] = subreddit_name\n",
    "    comments[\"subreddit\"] = subreddit_name\n",
    "    # Add the submissions and comments to the lists\n",
    "    submissions_list.append(submissions)\n",
    "    comments_list.append(comments)\n",
    "\n",
    "# Concatenate the submissions and comments dataframes\n",
    "all_submissions = pd.concat(submissions_list)\n",
    "all_comments = pd.concat(comments_list)\n",
    "\n",
    "# Simple analyze the data\n",
    "simple_analyze_submissions_and_comments(all_submissions, all_comments)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0d1fe3332fa8713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now look at the authors with the most comments.",
   "id": "dad20f641c60c31f"
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the users with most comments and the number of comments\n",
    "all_comments[\"author\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b4d0df42f422150",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now check if the top 100 authors with the most comments have a lot of duplicated comments.",
   "id": "b754b64384e7dbf5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will proceed with the analysis of the authors with most submissions (posts).",
   "id": "130414aab409d47c"
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the users with most submissions\n",
    "all_submissions[\"author\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1adebfa8909323de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now check the most common comments.",
   "id": "a836ede013d79d59"
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the most common comments\n",
    "all_comments[\"body\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca56ae07462a818",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's check the most common comments with more than 10 words and more than 1 occurrence.",
   "id": "5d6938b4acd028f7"
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the most common comments with more than 10 words and more than 1 occurrence add the comment ID too\n",
    "all_repeated_comments = all_comments[all_comments[\"body\"].apply(lambda x: len(str(x).split()) >= 10)][\"body\"].value_counts()[all_comments[all_comments[\"body\"].apply(lambda x: len(str(x).split()) >= 10)][\"body\"].value_counts() > 1]\n",
    "all_repeated_comments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e05e93ec5dac0c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will lastly plot the distribution of the scores for the comments.",
   "id": "8fa3adb7cd0b14ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the distribution of the scores for the comments in a log-log scale\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_comments[\"score\"], bins=100, log=True)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of the scores for the comments\")\n",
    "plt.show()"
   ],
   "id": "5463686440028ecc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see from plotting the distribution of the scores for the comments, many comments have a score of 0-10 (because they are new or not interesting). But also, there are thousands of comments with a scores higher than 100. This means that there are many popular/interesting comments in the dataset. We may use this score to perform weighted retrieval in the future.",
   "id": "eaad2793a40f451e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data Cleaning\n",
    "In this section, we will clean the data. This is important because we don't want to have duplicated comments, comments with less than a certain number of words, comments with a high percentage of special characters, etc.  \n",
    "\n",
    "First, we will define a function to clean the comments that:\n",
    "- Occur in all_repeated_comments and have more than 3 occurrences\n",
    "- Have less than a certain number of words\n",
    "- Are duplicated (have the same body and author)\n",
    "- Have a high percentage of special characters\n",
    "- Have low number of unique words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a6635b442f2d93c"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the function to clean the comments\n",
    "def clean_comments(subreddits, min_word_count):\n",
    "    subreddits_data = {}\n",
    "    for subreddit_name in subreddits:\n",
    "        # Load the submissions and comments dataframes\n",
    "        submissions, comments = load_submissions_and_comments(subreddit_name)\n",
    "        # Add the submissions and comments dataframes to the dictionary\n",
    "        subreddits_data[subreddit_name] = {\"submissions\": submissions, \"comments\": comments}\n",
    "        \n",
    "    # Loop through the subreddits\n",
    "    for subreddit_name, data in subreddits_data.items():\n",
    "        initial_count = len(data[\"comments\"])\n",
    "\n",
    "        # Remove the comments that occur in all_repeated_comments and have more than 3 occurrences\n",
    "        data[\"comments\"] = data[\"comments\"][~data[\"comments\"][\"body\"].isin(all_repeated_comments[all_repeated_comments > 3].index)]\n",
    "        print(f\"{subreddit_name}: Removed {initial_count - len(data['comments'])} comments that occur in all_repeated_comments and have more than 3 occurrences\")\n",
    "        initial_count = len(data[\"comments\"])\n",
    "\n",
    "        # Clear the comments with less than min_word_count words\n",
    "        data[\"comments\"] = data[\"comments\"][data[\"comments\"][\"body\"].apply(lambda x: len(str(x).split()) >= min_word_count)]\n",
    "        print(f\"{subreddit_name}: Removed {initial_count - len(data['comments'])} comments with less than {min_word_count} words\")\n",
    "        initial_count = len(data[\"comments\"])\n",
    "\n",
    "        # Remove duplicated comments with same body and author\n",
    "        data[\"comments\"] = data[\"comments\"].drop_duplicates(subset=[\"body\", \"author\"])\n",
    "        print(f\"{subreddit_name}: Removed {initial_count - len(data['comments'])} duplicated comments\")\n",
    "        initial_count = len(data[\"comments\"])\n",
    "\n",
    "        # Remove comments with high percentage of special characters\n",
    "        data[\"comments\"] = data[\"comments\"][data[\"comments\"][\"body\"].apply(lambda x: len([c for c in str(x) if not c.isalnum()]) / len(str(x)) < 0.5)]\n",
    "        print(f\"{subreddit_name}: Removed {initial_count - len(data['comments'])} comments with high percentage of special characters\")\n",
    "        initial_count = len(data[\"comments\"])\n",
    "\n",
    "        # Remove comments with less than 40% unique words\n",
    "        data[\"comments\"] = data[\"comments\"][data[\"comments\"][\"body\"].apply(lambda x: len(set(str(x).split())) / len(str(x).split()) > 0.4)]\n",
    "        print(f\"{subreddit_name}: Removed {initial_count - len(data['comments'])} comments with less than 40% unique words\")\n",
    "        \n",
    "\n",
    "        # Save the cleaned comments to a csv file\n",
    "        data[\"comments\"].to_csv(f'{data_directory}/{subreddit_name}/comments_cleaned_{min_word_count}.csv', index=False)\n",
    "\n",
    "        # Print the number of comments after cleaning\n",
    "        print(f\"{subreddit_name}: Number of comments after cleaning: {len(data['comments'])}\")\n",
    "        print()\n",
    "\n",
    "    # Print the total number of comments after cleaning\n",
    "    total_comments = sum([len(data[\"comments\"]) for data in subreddits_data.values()])\n",
    "    print(f\"Total number of comments after cleaning: {total_comments}\")\n",
    "\n",
    "    # Print the number of comments that were removed using\n",
    "    print(f\"Number of comments that were removed using all_comments: {len(all_comments) - total_comments}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68cc9dd88393d38c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we want our comments to be meaningful, we will select the comments that have more than 50 words. We will clean the comments using the function defined above.",
   "id": "4a76f33c13278965"
  },
  {
   "cell_type": "code",
   "source": [
    "# Clean with min_word_count = 50\n",
    "clean_comments(subreddits, 50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecad2069aa26b6a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the min_word_count variable\n",
    "min_word_count = 50"
   ],
   "id": "25aea4903f485485",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Combine the cleaned comments for all subreddits\n",
    "cleaned_comments_list = []\n",
    "for subreddit_name in subreddits:\n",
    "    # Load the cleaned comments\n",
    "    cleaned_comments = pd.read_csv(f'{data_directory}/{subreddit_name}/comments_cleaned_{min_word_count}.csv')\n",
    "    # Add the subreddit name to the cleaned comments\n",
    "    cleaned_comments[\"subreddit\"] = subreddit_name\n",
    "    # Add the cleaned comments to the list\n",
    "    cleaned_comments_list.append(cleaned_comments)\n",
    "\n",
    "# Concatenate all the cleaned comments into a single DataFrame\n",
    "all_cleaned_comments = pd.concat(cleaned_comments_list)\n",
    "\n",
    "# Save the cleaned comments to a csv file\n",
    "all_cleaned_comments.to_csv(f'{data_directory}/all/comments_cleaned_{min_word_count}.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1367d711a9fb9bc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the cleaned comments for all subreddits\n",
    "cleaned_comments = pd.read_csv(f'{data_directory}/all/comments_cleaned_{min_word_count}.csv')\n",
    "cleaned_comments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "340b70b1f78479dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Rename the score column to upvotes\n",
    "cleaned_comments = cleaned_comments.rename(columns={\"score\": \"upvotes\"})\n",
    "\n",
    "# Save the cleaned comments to a csv file\n",
    "cleaned_comments.to_csv(f'{data_directory}/all/comments_cleaned_{min_word_count}.csv', index=False)"
   ],
   "id": "17d60457203c7dd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will also combine all the submissions for all subreddits and save them to a csv file.",
   "id": "6cb736e3abc060e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d47b2434f110cea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the DataFrame to store the submissions for all subreddits\n",
    "submissions_list = []\n",
    "\n",
    "# Load the submissions for all subreddits\n",
    "for subreddit_name in subreddits:\n",
    "    # Load the submissions\n",
    "    submissions = pd.read_csv(f'{data_directory}/{subreddit_name}/submissions.csv')\n",
    "    # Add the subreddit name to the submissions\n",
    "    submissions[\"subreddit\"] = subreddit_name\n",
    "    # Add the submissions to the list\n",
    "    submissions_list.append(submissions)\n",
    "\n",
    "# Concatenate all the submissions into a single DataFrame\n",
    "all_submissions = pd.concat(submissions_list)\n",
    "\n",
    "# Save the submissions to a csv file\n",
    "all_submissions.to_csv(f'{data_directory}/all/submissions.csv', index=False)"
   ],
   "id": "fec1f80a7aa3bc22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we have all the cleaned comments, so will again collect their submissions.",
   "id": "9dc6afeee9068ac4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the set of unique submission IDs for the cleaned comments\n",
    "unique_submission_ids = set(all_cleaned_comments[\"link_id\"].unique())\n",
    "print(f\"Number of unique submission IDs: {len(unique_submission_ids)}\")\n",
    "\n",
    "# Use PRAW to get the submissions for the unique submission IDs\n",
    "submissions_list = []\n",
    "cnt = 0\n",
    "max = len(unique_submission_ids)\n",
    "unique_submission_ids = list(unique_submission_ids)\n",
    "for submission_id in unique_submission_ids:\n",
    "    submission = reddit.submission(id=submission_id[3:])\n",
    "    # Print error message if the submission is not found\n",
    "    if submission is None:\n",
    "        print(f\"Submission with ID {submission_id} not found\")\n",
    "        continue\n",
    "    new_submission = {\n",
    "        \"author\": submission.author,\n",
    "        \"created_utc\": submission.created_utc,\n",
    "        \"distinguished\": submission.distinguished,\n",
    "        \"id\": submission.id,\n",
    "        \"name\": submission.name,\n",
    "        \"num_comments\": submission.num_comments,\n",
    "        \"upvotes\": submission.score,\n",
    "        \"selftext\": submission.selftext,\n",
    "        \"title\": submission.title,\n",
    "        \"upvote_ratio\": submission.upvote_ratio,\n",
    "        \"url\": submission.url\n",
    "    }\n",
    "    submissions_list.append(new_submission)\n",
    "    # Add a progress bar that will stay on the same line\n",
    "    cnt += 1\n",
    "    print(f\"Submission {cnt}/{max} collected\", end=\"\\r\")\n",
    "    \n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "submissions = pd.DataFrame(submissions_list, columns=new_submission.keys())\n",
    "submissions"
   ],
   "id": "e63ff8daebea1bf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define final submission df a copy of the submissions df\n",
    "final_submissions = submissions.copy()\n",
    "# Convert the created_utc column date\n",
    "final_submissions[\"created_utc\"] = final_submissions[\"created_utc\"].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "# Save the submissions to a csv file\n",
    "final_submissions.to_csv(f'{data_directory}/all/submissions_cleaned_{min_word_count}.csv', index=False)"
   ],
   "id": "b7ae5575e90f925c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the comments with the classification\n",
    "class_comments = pd.read_csv('comments_class.csv')\n",
    "\n",
    "# Load the cleaned comments\n",
    "cleaned_comments = pd.read_csv(f'{data_directory}/all/comments_cleaned_{min_word_count}.csv')\n",
    "\n",
    "# Convert the created_utc column date\n",
    "cleaned_comments[\"created_utc\"] = cleaned_comments[\"created_utc\"].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "\n",
    "# Add the Prediction_Class and Confidence_Level columns to the cleaned comments (match by id)\n",
    "cleaned_comments = cleaned_comments.merge(class_comments[[\"id\", \"Predicted_Class\", \"Confidence_Level\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "# Save the cleaned comments with the classification to a csv file\n",
    "cleaned_comments.to_csv(f'{data_directory}/all/comments_cleaned_{min_word_count}_class.csv', index=False)"
   ],
   "id": "dfee33f1b34d7b36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1639057bcad58d2a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
