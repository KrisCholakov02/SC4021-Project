{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Solr Data Preprocessing",
   "id": "5aa32677915f76dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Synonyms\n",
    "In this section, we will get the synonyms of the terms in the comments. First we will need to import the necessary libraries and download the WordNet dataset."
   ],
   "id": "8ea72b4910f739e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we need to define that we will use synonyms",
   "id": "251d37902104d256"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ],
   "id": "caab85abf43e1f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8983/solr/comments/schema/analysis/synonyms/en\"\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"class\": \"org.apache.solr.rest.schema.analysis.ManagedSynonymGraphFilterFactory$SynonymManager\"\n",
    "})\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "print(response.text)"
   ],
   "id": "db58ef3b6aef57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8983/solr/submissions/schema/analysis/synonyms/en\"\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"class\": \"org.apache.solr.rest.schema.analysis.ManagedSynonymGraphFilterFactory$SynonymManager\"\n",
    "})\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "print(response.text)"
   ],
   "id": "59cb9614e589bea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "nltk.download('wordnet')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After, setting up the required libraries, we can continue with getting the unique terms from the comments in the DataFrame.",
   "id": "5ea279bca2c9f583"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the set to hold the words\n",
    "comment_terms = set()\n",
    "submission_terms = set()\n",
    "\n",
    "# Read the comments from the csv\n",
    "comments = pd.read_csv('./subreddits/all/comments_cleaned_50.csv')\n",
    "\n",
    "# Read the submissions from the csv\n",
    "submissions = pd.read_csv('./subreddits/all/submissions.csv')\n",
    "\n",
    "# Iterate over the comments\n",
    "for index, row in comments.iterrows():\n",
    "    # Split the comment into words\n",
    "    words = row['body'].split()\n",
    "    # Add each word to the set of terms\n",
    "    for w in words:\n",
    "        comment_terms.add(str.lower(w))\n",
    "\n",
    "# Iterate over the submissions\n",
    "for index, row in submissions.iterrows():\n",
    "    words = []\n",
    "\n",
    "    # Check if 'selftext' is not NaN and then split into words\n",
    "    if pd.notna(row['selftext']):\n",
    "        words.extend(row['selftext'].split())\n",
    "\n",
    "    # Check if 'title' is not NaN and then split into words\n",
    "    if pd.notna(row['title']):\n",
    "        words.extend(row['title'].split())\n",
    "    # Add each word to the set of terms\n",
    "    for w in words:\n",
    "        submission_terms.add(str.lower(w))"
   ],
   "id": "4b8eea2661198c29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the synonyms of the words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "comments_synonyms = {}\n",
    "submissions_synonyms = {}\n",
    "\n",
    "for word in comment_terms:\n",
    "    comments_synonyms[word] = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            comments_synonyms[word].append(lemma.name())\n",
    "\n",
    "for word in submission_terms:\n",
    "    submissions_synonyms[word] = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            submissions_synonyms[word].append(lemma.name())\n",
    "\n",
    "# remove the ones with empty list values\n",
    "comments_synonyms = {k: v for k, v in comments_synonyms.items() if v}\n",
    "\n",
    "# remove the ones with empty list values\n",
    "submissions_synonyms = {k: v for k, v in submissions_synonyms.items() if v}\n"
   ],
   "id": "caaa3929bda709",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the synonyms as a JSON file\n",
    "with open('comments_synonyms.json', 'w') as f:\n",
    "    json.dump(comments_synonyms, f)\n",
    "with open('submissions_synonyms.json', 'w') as f:\n",
    "    json.dump(submissions_synonyms, f)"
   ],
   "id": "4767669e6df5749b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the synonyms from the JSON file\n",
    "with open('comments_synonyms.json') as f:\n",
    "    comments_synonyms = json.load(f)\n",
    "with open('submissions_synonyms.json') as f:\n",
    "    submissions_synonyms = json.load(f)\n",
    "\n",
    "# Send request to the Solr server to add the synonyms\n",
    "comments_url = \"http://localhost:8983/solr/comments/schema/analysis/synonyms/en\"\n",
    "submissions_url = \"http://localhost:8983/solr/submissions/schema/analysis/synonyms/en\"\n",
    "comments_payload = json.dumps(comments_synonyms)\n",
    "submissions_payload = json.dumps(submissions_synonyms)\n",
    "headers = {'Content-type': 'application/json'}\n",
    "c_response = requests.request(\"PUT\", comments_url, headers=headers, data=comments_payload)\n",
    "s_response = requests.request(\"PUT\", submissions_url, headers=headers, data=submissions_payload)\n",
    "print(c_response.text)\n",
    "print(s_response.text)"
   ],
   "id": "35feaf3721c4889b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's create the necessary fields",
   "id": "53e7f58765f7e09f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = 'http://localhost:8983/solr/submissions/schema'\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = {\n",
    "    \"add-field\": {\n",
    "        \"name\": \"title\",\n",
    "        \"type\": \"sgm_text_en\",\n",
    "        \"stored\": True,\n",
    "        \"indexed\": True,\n",
    "        \"multiValued\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.text)"
   ],
   "id": "6ad80dd248764da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = 'http://localhost:8983/solr/comments/schema'\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = {\n",
    "    \"add-field\": {\n",
    "        \"name\": \"body\",\n",
    "        \"type\": \"sgm_text_en\",\n",
    "        \"stored\": True,\n",
    "        \"indexed\": True,\n",
    "        \"multiValued\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.text)"
   ],
   "id": "d588128f1b517367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we will upload the files",
   "id": "d2097737760115a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# URL of the Solr update endpoint\n",
    "url = 'http://localhost:8983/solr/submissions/update'\n",
    "\n",
    "# Headers to specify that the payload is XML\n",
    "headers = {'Content-Type': 'text/csv'}\n",
    "\n",
    "# Load the XML data from a file\n",
    "with open('subreddits/all/submissions_cleaned_50.csv', 'rb') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Send the POST request with the XML data\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "# Print the HTTP response text\n",
    "print(response.text)"
   ],
   "id": "224e866da0b0c65d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# URL of the Solr update endpoint\n",
    "url = 'http://localhost:8983/solr/comments/update'\n",
    "\n",
    "# Headers to specify that the payload is XML\n",
    "headers = {'Content-Type': 'text/csv'}\n",
    "\n",
    "# Load the XML data from a file\n",
    "with open('./subreddits/all/comments_cleaned_50_class.csv', 'rb') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Send the POST request with the XML data\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "# Print the HTTP response text\n",
    "print(response.text)"
   ],
   "id": "a20821ebb7a8e851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "93bd926c2e67bda9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
