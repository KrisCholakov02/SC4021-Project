{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Solr Data Preprocessing",
   "id": "5aa32677915f76dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Synonyms\n",
    "In this section, we will get the synonyms of the terms in the comments. First we will need to import the necessary libraries and download the WordNet dataset."
   ],
   "id": "8ea72b4910f739e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we need to define that we will use synonyms",
   "id": "251d37902104d256"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T02:16:53.258162Z",
     "start_time": "2024-04-14T02:16:52.533302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from nltk.corpus import wordnet"
   ],
   "id": "caab85abf43e1f0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8983/solr/comments/schema/analysis/synonyms/en\"\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"class\": \"org.apache.solr.rest.schema.analysis.ManagedSynonymGraphFilterFactory$SynonymManager\"\n",
    "})\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "print(response.text)"
   ],
   "id": "db58ef3b6aef57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = \"http://localhost:8983/solr/submissions/schema/analysis/synonyms/en\"\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"class\": \"org.apache.solr.rest.schema.analysis.ManagedSynonymGraphFilterFactory$SynonymManager\"\n",
    "})\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "print(response.text)"
   ],
   "id": "59cb9614e589bea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "nltk.download('wordnet')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After, setting up the required libraries, we can continue with getting the unique terms from the comments in the DataFrame.",
   "id": "5ea279bca2c9f583"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the set to hold the words\n",
    "comment_terms = set()\n",
    "submission_terms = set()\n",
    "\n",
    "# Read the comments from the csv\n",
    "comments = pd.read_csv('./subreddits/all/comments_cleaned_50.csv')\n",
    "\n",
    "# Read the submissions from the csv\n",
    "submissions = pd.read_csv('./subreddits/all/submissions.csv')\n",
    "\n",
    "# Iterate over the comments\n",
    "for index, row in comments.iterrows():\n",
    "    # Split the comment into words\n",
    "    words = row['body'].split()\n",
    "    # Add each word to the set of terms\n",
    "    for w in words:\n",
    "        comment_terms.add(str.lower(w))\n",
    "\n",
    "# Iterate over the submissions\n",
    "for index, row in submissions.iterrows():\n",
    "    words = []\n",
    "\n",
    "    # Check if 'selftext' is not NaN and then split into words\n",
    "    if pd.notna(row['selftext']):\n",
    "        words.extend(row['selftext'].split())\n",
    "\n",
    "    # Check if 'title' is not NaN and then split into words\n",
    "    if pd.notna(row['title']):\n",
    "        words.extend(row['title'].split())\n",
    "    # Add each word to the set of terms\n",
    "    for w in words:\n",
    "        submission_terms.add(str.lower(w))"
   ],
   "id": "4b8eea2661198c29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comments_synonyms = {}\n",
    "submissions_synonyms = {}\n",
    "\n",
    "for word in comment_terms:\n",
    "    comments_synonyms[word] = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            comments_synonyms[word].append(lemma.name())\n",
    "\n",
    "for word in submission_terms:\n",
    "    submissions_synonyms[word] = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            submissions_synonyms[word].append(lemma.name())\n",
    "\n",
    "# remove the ones with empty list values\n",
    "comments_synonyms = {k: v for k, v in comments_synonyms.items() if v}\n",
    "\n",
    "# remove the ones with empty list values\n",
    "submissions_synonyms = {k: v for k, v in submissions_synonyms.items() if v}\n"
   ],
   "id": "caaa3929bda709",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the synonyms as a JSON file\n",
    "with open('comments_synonyms.json', 'w') as f:\n",
    "    json.dump(comments_synonyms, f)\n",
    "with open('submissions_synonyms.json', 'w') as f:\n",
    "    json.dump(submissions_synonyms, f)"
   ],
   "id": "4767669e6df5749b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the synonyms from the JSON file\n",
    "with open('comments_synonyms.json') as f:\n",
    "    comments_synonyms = json.load(f)\n",
    "with open('submissions_synonyms.json') as f:\n",
    "    submissions_synonyms = json.load(f)\n",
    "\n",
    "# Send request to the Solr server to add the synonyms\n",
    "comments_url = \"http://localhost:8983/solr/comments/schema/analysis/synonyms/en\"\n",
    "submissions_url = \"http://localhost:8983/solr/submissions/schema/analysis/synonyms/en\"\n",
    "comments_payload = json.dumps(comments_synonyms)\n",
    "submissions_payload = json.dumps(submissions_synonyms)\n",
    "headers = {'Content-type': 'application/json'}\n",
    "c_response = requests.request(\"PUT\", comments_url, headers=headers, data=comments_payload)\n",
    "s_response = requests.request(\"PUT\", submissions_url, headers=headers, data=submissions_payload)\n",
    "print(c_response.text)\n",
    "print(s_response.text)"
   ],
   "id": "35feaf3721c4889b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's create the necessary fields",
   "id": "53e7f58765f7e09f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = 'http://localhost:8983/solr/submissions/schema'\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = {\n",
    "    \"add-field\": {\n",
    "        \"name\": \"title\",\n",
    "        \"type\": \"sgm_text_en\",\n",
    "        \"stored\": True,\n",
    "        \"indexed\": True,\n",
    "        \"multiValued\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.text)"
   ],
   "id": "6ad80dd248764da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = 'http://localhost:8983/solr/comments/schema'\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = {\n",
    "    \"add-field\": {\n",
    "        \"name\": \"body\",\n",
    "        \"type\": \"sgm_text_en\",\n",
    "        \"stored\": True,\n",
    "        \"indexed\": True,\n",
    "        \"multiValued\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.text)"
   ],
   "id": "d588128f1b517367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we will upload the files",
   "id": "d2097737760115a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T14:32:26.443057Z",
     "start_time": "2024-04-13T14:32:25.148424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# URL of the Solr update endpoint\n",
    "url = 'http://localhost:8983/solr/submissions/update'\n",
    "\n",
    "# Headers to specify that the payload is XML\n",
    "headers = {'Content-Type': 'text/csv'}\n",
    "\n",
    "# Load the XML data from a file\n",
    "with open('subreddits/all/submissions_cleaned_50.csv', 'rb') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Send the POST request with the XML data\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "# Print the HTTP response text\n",
    "print(response.text)"
   ],
   "id": "224e866da0b0c65d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"responseHeader\":{\n",
      "    \"status\":0,\n",
      "    \"QTime\":1267\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T02:38:57.711332Z",
     "start_time": "2024-04-14T02:38:02.412161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# URL of the Solr update endpoint\n",
    "url = 'http://localhost:8983/solr/comments/update'\n",
    "\n",
    "# Headers to specify that the payload is XML\n",
    "headers = {'Content-Type': 'text/csv'}\n",
    "\n",
    "# Load the XML data from a file\n",
    "with open('./subreddits/all/comments_cleaned_50_class.csv', 'rb') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Send the POST request with the XML data\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "# Print the HTTP response text\n",
    "print(response.text)"
   ],
   "id": "a20821ebb7a8e851",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"responseHeader\":{\n",
      "    \"status\":0,\n",
      "    \"QTime\":55255\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Lemmatization\n",
    "Now we will lemmatize the terms in the comments and add them to the synonyms."
   ],
   "id": "57faf555c920e340"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T02:31:39.405769Z",
     "start_time": "2024-04-14T02:31:38.343576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the synonyms JSON file\n",
    "with open('comments_synonyms.json') as f:\n",
    "    comments_synonyms = json.load(f)\n",
    "    \n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the terms\n",
    "for term in comments_synonyms:\n",
    "    lemmas = []\n",
    "    for syn in comments_synonyms[term]:\n",
    "        lemma = lemmatizer.lemmatize(syn)\n",
    "        if lemma not in lemmas:\n",
    "            lemmas.append(lemma)\n",
    "    comments_synonyms[term].extend(lemmas)\n",
    "    # Add the term itself to the list of synonyms\n",
    "    comments_synonyms[term].append(term)\n",
    "    # Remove duplicates\n",
    "    comments_synonyms[term] = list(set(comments_synonyms[term]))\n",
    "    \n",
    "# Save the synonyms as a JSON file\n",
    "with open('comments_synonyms_lemmatized.json', 'w') as f:\n",
    "    json.dump(comments_synonyms, f)"
   ],
   "id": "93bd926c2e67bda9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_processor', 'reckoner', 'electronic_computer', 'computer', 'computers', 'figurer', 'information_processing_system', 'computing_machine', 'calculator', 'computing_device', 'estimator']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T02:32:50.636905Z",
     "start_time": "2024-04-14T02:32:50.273714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Send request to the Solr server to add the synonyms\n",
    "comments_url = \"http://localhost:8983/solr/comments/schema/analysis/synonyms/en\"\n",
    "comments_payload = json.dumps(comments_synonyms)\n",
    "headers = {'Content-type': 'application/json'}\n",
    "c_response = requests.request(\"PUT\", comments_url, headers=headers, data=comments_payload)\n",
    "print(c_response.text)"
   ],
   "id": "7786892e9b57bfb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"responseHeader\":{\n",
      "    \"status\":0,\n",
      "    \"QTime\":195\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b653f76a48b2f4b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
